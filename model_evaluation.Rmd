---
title: "Model Evaluation for Time-to-Event Studies"
author: "Daniel Saggau"
date: "11/22/2020"
fontsize: 11pt
output:
   pdf_document:
    number_sections: true
citation_package: --biblatex
bibliography: [references.bib]
nocite: '@*'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include =F}
library("mlr3")
library("mlr3learners")
library("mlr3proba")
library("mlr3viz")
library("pec")
library("prodlim")
```

# Introduction 

A common challenge of Time-to-Event studies is dealing with censored data.
Censored data implies that we don't have information on the event for all subjects in our data. 
Reasons for that can be manifold.
One example would be that the study ends prior to the event occurring.
This paper will illustrate some model evaluation metrics and respective modifications for Time-to-Event studies.
Here, the focus is predominately on popular extensions of the loss function and the area under the curve(AUC), specifically drawing attention to the IBS and the concordance-index or in short c-index.

The c-index does enjoy prominence among clinicians due to interpretability and the ability to make insightful conclusions for individual subjects.
Essentially,the c-index only considers discrimination, neglecting model calibration.
When trying to measure discrimination alone, it is the most popular tool of choice.
The integrated brier score is the preferable tool for model evaluation for overall performance looking specifically at machine learning methods given the relative importance of calibration in predictive modelling.

The paper is structured as follows:
Firstly, there is an introduction of the different components of model evaluation.
The subsequent sections outline the two dominant methods, the IBS and the c-index for Time-to-Event studies.
The section thereafter will provide a brief outline of some novel complementary methods.
Lastly, the conclusion will summarize core findings of this brief outline. 

# Components of Model Evaluation 

When evaluating model performance, one needs to differentiate between the type of study at hand.
In a clinical setting, studies can be separated into diagnostic and prognostic studies.

## Diagnostic and Prognostic studies 

Diagnostic studies are concerned with the problem of how to classify a patient at that very point in time.
For binary classification tasks during diagnostic studies, where we need separate between e.g. patients with and without disease, optimal discrimination is a pivotal concern. 
Prognostics on the other hand deals with predictive modeling were also accuracy becomes an eminent consideration. 
Diagnostic is of less interest for the field of machine learning.

Further, we can disentangle the different components of model evaluation into various groups.
Fundamentally, model evaluation focuses on discrimination and calibration.
A third more recent focus is the issue of clinical usefulness.
Clinical usefulness is as the name suggests only relevant for clinical research and henceforth of secondary focus here. 

## Discrimination 

When controlling for discrimination, we are controlling for how well our model is handling subjects with outcomes as compared to subjects without outcomes. 
Therefore, as the name suggest, we are testing how strong our model discriminates between subjects that incur an event versus subjects that don't. 
Perfect discrimination would imply that all our subjects with the outcome have higher scores than subjects that do not have an outcome.
When solely using a discrimination centered evaluation tool, our predictive accuracy could be severely impaired, but as long as we discriminate correctly we would still measure a strong performance. 
The most prominent tools to study discrimination is the concordance statistics. 

### Concordance-statistics, Harell's C and the c-index

The  Receiver Operating Characteristic Curve (ROC), the curve in the area under the curve (AUC) score, is an important model evaluation tool for discrimination.
Essentially, the ROC allows one to account for imbalanced label distribution and imbalanced misclassification costs.
Because of these factors, we need to account for more than solely accuracy of our model and we want to look at the model performance over various default thresholds rather than at a specific misclassification specification.
The ROC is the foundation for the concordance statistics (c-statistics), a modified AUC suitable for censored data (Antolini et al., 2005).
Concordance describes consistency.
Prior to discussing the c-index, this section will briefly recap the foundation of the ROC and AUC.
In a nutshell, the ROC takes into account two factors namely sensitivity and specificity.
The ROC takes these two factors and plots sensitivity against (1- specificity). 
The area under the curve or the c-statistic ranges from 0.5 (no discrimination) to the maximum value of 1 (perfect discrimination).

**Sensitivity**:
Firstly, sensitivity deals with values above the threshold among the subject group which do endure an event e.g. the subjects with diseases (Cook, N. 2007).
Another common name for Sensitivity is the true positive rate.

$$
TPF = \frac{TP}{TP+FN}
$$

**Specificity**:
Specificity deals with false negatives, hence patients with a disease we classify as not having any diseases.
Another name for specificity is the true negative rate. 

$$
TNR = \frac{TN}{TN+FP}
$$

#### From AUC/ROC to concordance statistics

The c- index is the generalization of the ROC for survival data (Cook, N., 2007).
Esentially, the difference can be written down as follows:

$$\mathrm{AUC = Pr(Risk_t(i)> Risk_t(j)| i\: has\: event\: before\: t\: and \: j \: has\: event\: after \: t})$$

In the AUC, we need uncensored data, because we need information on both subjects.
With the c-index, we only need one of two subjects to have an event taking place for the subjectpair to be comparable.

$$\mathrm{C = Pr(Risk_t(i)> Risk_t(j)| i\: has\: event\: before\: t)}$$

Due to the fact that we deal need to be able to deal with censored data, we need to modify the AUC.
The c-statistic describes how well models rank case and non-case, using a rank correlation measure, based on Kendall's tau (Uno et al., 2011).
Generally speaking, a c-statistics above the threshold of 0,8 can be considered good (Zhang et al.,2018).
A concordance pair is a pair that is consistent, henceforth subjects with higher risks have earlier events and subjects with lower risk scores translate in later event time points.

$$\mathrm{\frac{Concordant\: Pairs}{Concordant\: Pairs + Discordant\: Pairs}}$$

Together concordant pairs (consistent) and discordant pairs(inconsistent) are classified as everything that is comparable. 
For subjectpairs to be comparable, we need at least one of the two subjects to have an event. 

#### Mathematical derivation 

$$
AUC^{I,D}(t) = P(X_i>c | T_i>t) \tag{5}
$$

Resulting in: 

$$
C^T = \int^T_0 AUC^{I,D}(t)w^T(t)dt \tag{6}
$$

### Modifications 

Harell'c is the most commonly used C-index. 
Antolini et al. (2005) propose a time dependent c-index for time dependent covariates.
They use a unique definition of concordance, arguing that any event that is not in the data, is bound to take place at a later point than any event that is already in the dataset (right censoring).
Their model considered the presence of a population feature rather than a shortcoming of the sample. 
Uno et al.(2011) propose a modified c-statistic which is consistent for population concordance measures.

Another popular method is the the integrated brier score, a score that controls for both discrimination and calibration.

## Calibration Plot 

We can use calibration plots to visualize the calibration of our model.
The 'pec' packages provides the 'calPlot' function.

```{r, include =F}
set.seed(18713)
library(prodlim)
library(survival)
library(pec)
dat=SimSurv(100)
pmodel=coxph(Surv(time,status)~X1+X2,data=dat,x=TRUE,y=TRUE)
perror=pec(list(Cox=pmodel),Hist(time,status)~1,data=dat)
## cumulative prediction error
y<- crps(perror,times=1) 
plot(perror)
```

```{r}
calPlot(pmodel)
```

# Brier Score

The score brier was initially used for weather forecasting (Graf et al., 1999).
With uni-dimensional predictions the brier score is the same as the mean squared error. 

## Mean Squared Error/ Loss Function 

MSE in a Regression setting:

$$\mathrm{MSE}=\frac{1}{n}\sum^n_{i=1}(y^{(i)})-\hat{y}^{(i)})^2$$ 

The Brier Score is the MSE for Classification: 

$$\mathrm{BS}= \frac{1}{n}\sum^n_{i=1}(\hat{\pi}(x^{(i)})-y^{(i)})^2$$ 

Other terminology that you might encounter is the predicted error or mean squared loss function (Schoop et al.,2011; Gerds & Schumacher, 2006).
The 'pec' package for instance refers to to the score as the predicted error curve. 
The mlr3proba package refers to the brier score as the 'surv.graf', based on Graf who initially modified the measure. 
## Explanation of Method 

The mean squared error in a nutshell is the incurred quadratic loss, studying the predicted and the true event status (Schoop et al.,2011).
Graf et al. (1999) state that the "...expected brier score may be interpreted as a mean squared error of prediction when the estimated prob, which take values in interval [0,1] are viewed as prediction of event status at t*,I(T>t*) in {0,1}."
The brier score is dependent on the evaluation time.
By introducing a reweighing scheme, one derives quantities that are independent on the censoring distribution and hence suitable for censored data (Graf et al.,1999).
To get a comprehensive time dependent model performance, multiple time points have to be studied.

For the individual at time t:

$$L(S,t|t^*) = [(S(t^*)^2)I(t)\leq t^*, \delta =1)(\frac{1}{G(t)})] + [((1-S(t^*))^2)I(t>t^*)(\frac{1}{G(t^*)})] \tag{10}$$

Integrated population mean version: 

$$L(S) = \frac{1}{NT}\displaystyle\sum^N_{i=1}\displaystyle\sum ^T_{j=1}L(S_i,t_i|t^*) \tag{11}$$

### (Integrated) Log loss survival measure/ (Integrated) cross entropy

**insert here** 

```{r, include =F}
set.seed(130971)
dat <- SimSurv(1000)

?SimSurv
# fit some candidate Cox models and compute the Kaplan-Meier estimate 

Models <- list("Cox.X1"=coxph(Surv(time,status)~X1,data=dat,x=TRUE,y=TRUE),
               "Cox.X2"=coxph(Surv(time,status)~X2,data=dat,x=TRUE,y=TRUE),
               "Cox.X1.X2"=coxph(Surv(time,status)~X1+X2,data=dat,x=TRUE,y=TRUE))

# compute the apparent prediction error 
PredError <- pec(object=Models,
                 formula=Surv(time,status)~X1+X2, data=dat,
                 exact=TRUE, cens.model="marginal", splitMethod="none",
                 B=0,
                 verbose=TRUE)


print(PredError,times=seq(1,30,1))
summary(PredError)
plot(PredError,xlim=c(0,20), ylim = c(0,0.5))
```

## mlr3 implementation 

Sonabend et al. (2020) provide a package for the mlr3 framework, namely mlr3proba.
An useful component is the benchmarking feature of different model evaluation measures.
The mlr3proba entails 5 different measures directly namely: 

* van Houwelingen’s Alpha Calibration
* van Houwelingen’s Beta Calibration
* Integrated Graf Score
* Integrated Log Loss
* Log Loss

# Complemenary Model Evaluation Metrics

## Net Reclassification improvement

Cook (2008) advocates for the usage of net reclassification improvement (NRI) and calibration tests for cross classified categories to study the clinical usefulness. 
While NRI is only a measure to study discrimination, it allows to account for the formation of categories based on clinical risk estimates.
Henceforth, reclassification complements existing clinicians in practical applications as opposed to providing a dominant model evaluation tools.
Integrated discrimination improvement (IDI) is equivalent to testing whether the regression coefficient in a model is equal to zero Cook, N. R., & Ridker, P. M. (2009) (somewhat similar to a R^2 score).
Cook  and Ridker (2009) point out that there is a dependency between reclassification measures and the categories used.
Further they suggest that reclassification calibration statistic and NRI both may be useful to demonstrate the ability of new models and markers when altering risk strata.

### Net reclassification and integrated discrimination improvement implementation 

Notable packages: 'survIDINRI': 

## Decision Analysis Curve 

Decision analysis curve enables the use of weights, allowing optimal decision making based on subjective preferences, embodied in a net benefit equation.
Further Vickers et al. (2016) illustrate that harm is transformed, using an exchange rate to put harm and benefit on one scale.
This exchange rate can be obtained by asking clinicians questions based on their subjective preferences such as how many patients they would have undergo a biopsy prior to finding a cancer or weighing the benefits of getting early findings as opposed to the cost of harmful further testing.
Together these elements build the net-benefit equation.
Plotting different exchange rates with the net benefit equation, gives us the decision analysis curve. 
The curves enable the practitioner the identification of the rage of threshold probabilities for when a model would be of value, providing information on the necessary benefits needed for a model to be useful and which of many models is optimal (Vickers, A., Elkin, E., 2006).
One important consideration is that decision analysis curve is a complement, not a substitute to existing models (Vickers, A., Elkin, E., 2006).

# Discussion 

## C-index
###  Advantages 

**Interpretation**
The c-index has gained popularity because of it's interpretability (Kattan and Gerds, 2018).
Especially for the individual patient in diagnostic studies, this method has gained popularity.

**Established**
Further, there are many well established packages in R to work with the AUC/ concordance statistics /c-index due to its popularity. 

### Disadvantages 

Prognostic studies need to account for the model accuracy which is measured by model calibration.
Calibration captures the accuracy of our predictions of our model which is very important for prediction models.
One ways to measure calibration is for instance the Hosmer-Lemeshow test, the "goodness of fit" test. (Gerds and Schumacher, 2006).
Kattan and Gerds (2018), argue that model evaluation metrics needs to be able to differentiate between useless and harmful models.
Harmful models are models that make severely wrong predictions (and some right ones) while useless models could e.g. always predict some level of prevalence. 
Henceforth, using a concordance statistic for prognostic studies is not advised.

**Estimators can be influenced by data:**
As mentioned above, for a more nuanced prevalence of a disease, the sensitivity is impaired (Cook,N., 2007). 
Specificity is dependent on the data structure, but as suggested by Cook (2007), specificity is for instance affected by age, gender and the prevalence of concomitant risk factors.

**Impaired Sensitivity:**
Because c-statistics is based on ranks it is less sensitive than e.g. measures based on probabilities.  

**Clinical consequences:**
The c-index does not account for clinical consequences and the subjective importance of false positives relative to false negatives. 

##IBS

### Advantages  
**Hollistic and Concise Statistical Underpinnings:**
The integrated brier score is a measure accounting for both discrimination and calibration separately.
Henceforth, it is more holistic tool for model evaluation.
Further, Graf et al. (1999) argue that the method is more sophisticated than the c-index because it deals with probabilities allowing us insights into the accuracy of our predictions rather than (mis-)classifications.  
As mentioned, the integrated brier score has the ability to differentiate between useless and harmful models. 

### Disadvantages 
**Dependency on Outcome prevalence:**
Kattan and Gerds (2018) suggest that the evaluation is somewhat problematic with respect to numerous aspects.
The benchmark of the different models are dependent on the overall prevalence of the event in our data set. 
Henceforth, when working with data where the event rarely takes place, the benchmark becomes convoluted (Kattan and Gerds, 2018).  
**Interpretation:**
One pivotal shortcoming of the method is the inability to compare results independent from other models.
Hence, one is at best only able to see that the one method is superior to the other models at hand. 

**Clinicial consequences**
Clinicians usually don't value the different components of model evaluation equally as their clinical consequences are not equivalent.
Further, we are unable to see whether the implementation of the model is advisable in the first place.
Steyerberg et al. (2010) argue that one is unable to detect whether the implementation will cause more harm than benefit.
Therefore, some scholars have advocated for complementary tests accounting for clinical consequences.

# Conclusion 

Time-to-Event studies require adjusted model evaluation tools for censored survival data.
At the core, studies separate between models that evaluate overall performance, discrimination and calibration.
Both the c-index for discrimination, and the IBS for discrimination and calibration, are well established tools to undertake model evaluation.
New methods such as reclassification and clinical usefulness have gained prominence among scholarship.

# References 
