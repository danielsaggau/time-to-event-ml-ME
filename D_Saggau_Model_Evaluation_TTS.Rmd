---
title: "Model Evaluation for Time to Event Studies"
author: "Daniel Saggau"
date: "11/15/2020"
fontsize: 11pt
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Introduction 

Time to event studies:

* What is a time to event study?

Time to event studies have gained prominence in a variety of fields, but predominately found audience within the field of medical research.
Nevertheless, areas of application are manifold and include e.g. financial statistics, examining the survival of an institution. 
One feature of time to event studies is the unique data structure.
A common characteristic of survival data is the right censored nature of observations.
Right censored data means that not every subject in the study experiences the event of interest.
The reasons for that can be various but one frequent reason for this is that the study ends prior to the event occurring.
But this could also happen because we have subjects dropping out of our study, which might not be a common feature in financial statistics but occurs with considerable frequency in clinical or epidemiological studies. 

Generally speaking, there are various model evaluation metrics.
The model choice is heavily dependent on the task at hand.
One way to categories models is e.g. separating them by diagnostic and prognostic.

One distinguishes between discrimination, calibration, 
Two prominent tools are the Receiver Operating Characteristic Curve or in short the ROC, and the c-statistics.
This paper will introduce a number of methods, focusing on popular extensions of these prominent tools adjusted for survival studies namely the brier score and the c-index, two methods that have gained prominence within the realms of scholarship and among clinicians.
Generally speaking, both measures, the c-index and the integrated brier score have their respective advantages and their unique merits have let these methods gained more attention than other methods.
While the c-index does enjoy considerably prominence among clinicians due to interpretability and the ability to make comprehensive conclusions for the individual patients.
Irrespective, the c-index only considers discrimination and does not account for calibration as an evaluation criterion which i will argue is a pivotal shortcoming when assessing different prognostic models and especially when working with machine learning methods in time to event studies.
Inevitably, this paper suggests that the integrated brier score is the more holistic model evaluation metric from the stance of a statistician. 

The paper is structured as follows:
Firstly, i will introduce the integrated brier score, providing further information on the origin, theoretical underpinning and the respective characteristics.
Secondly, i will introduce the c-index, following the same procedure as with the integrated brier score.
Following the analysis of these two cornerstones of model evaluation for time to event studies, i will illustrate further competing methods and current research within the field. 
Subsequently, the fourth section will provide a brief example of how to implement given methods in R.
Lastly, the conclusion will summarize core findings of this brief comparative study. 

# Model Evaluation metrics 

When attempting to evaluate a model performance, there are various approaches.
The optimal model evaluation metric inevitably will depend not only on the target but also the target audience at hand. 
Broadly speaking, various scholars suggest that the most rudimentary distinction of model evaluation metrics is dependent on the ability to capture discrimination, "do patients who have outcome have higher risk predictions than those who dont" and model calibration, **"Measure how well predicted probabilities agree with actual observed risk".**. 
I will briefly introduce these concepts. 
Additionally to these pivotal considerations, scholarship and clinicians have also developed metrics that focus on decision analytics and reclassification measures. 
These tools are less prominent, but will be mentioned as they also carry merit. 

## Discrimination 

Discrimination is the ability of a model to handle patients that do not have outcomes accordingly.
Inevitably, when controlling for discrimination, we are controlling for how well our model is handling subjects with outcomes as compared to subjects without outcomes. 
Therefore, as the name suggest, we are testing how strong our model discriminates between subjects that incur an event versus subjects that dont. 
As a frequent property of survival data sets is the fact that have right censured data, data that entails subjects without the outcome/event taking place.
Henceforth, discrimination is an important pillar for model evaluation in survival analysis. 
Various measures have emerged, that deal with discrimination such as the c-index. 
Perfect discrimination would imply that all our subjects with the event (e.g. a disease) have higher scores than subjects that do not have an event within their time period.
One should note that when using a model that only controls for discrimination, our predictive accuracy could be horrible but as long as this condition holds, inaccurate models could be evaluated falsely as superior. 

## Calibration 

One should mention that especially when actually applying these methods, in a clinical setting one either deals with diagnostic or prognostic tasks.
Diagnostic is the analysis of a given subject at that point in time.
For binary classification tasks during diagnostic studies, where we need separate between e.g. patients with and without disease, discrimination is a very important concern and potentially of greater importance. 
Prognostic on the other hand deals with predictive modeling, predicting e.g. in our surivival analysis setting the survival of a patient.

When dealing with an prognostic analysis, calibration can become an important concern. 
Calibration captures the accuracy of our predictions of our model.
The underlying goal is to ensure that the predictions are as accurate as possible.
For this very reason, the research community has highlighted the added value of using e.g. the integrated brier score, a score that controls for both discrimination and calibration.
To further understand these methods, the following section will briefly introduce the origins of these methods, namely the ROC curve and the brier score.


### Origin ROC / Concordance -statistics

The  Receiver Operating Characteristic Curve (ROC) is an important model evaluation tool, gaining substantial prominence in various fields of statistics.
This method is the foundation of the c-index which is one of the most prominent tools within the field of model evaluation for survival analysis. 
In a nutshell, the ROC takes into account two factors namely sensitivity and specificity. 
Firstly, Sensitivity deals with the likelihood of positive test results, specifically it deals with values above the threshold among the subject group which do endure an event e.g. the subjects with diseases (Cook, N. 2007).
Sensitivity becomes more volatile when for instance dealing with milder, nuanced cases of a disease. 
On the other hand, specificity deals with false negatives, patients with a disease we classify as not having any diseases. 
However, specificity is especially subject to the influence of the characteristics of a subject without disease.
Examples of such characteristics are age or gender. 
The ROC takes these two factors and plots sensitivity against 1- specificity. 


* The area under the curve or the c statistic ranges from 0.5 (no discrimination) to max of 1 (perfect discrimination)
* Essentially, the c statistic is equivalent to the probability that the measure or predicted risk is higher for a case than for a non-case. 
* Further, c-statistic describes how well models rank case and noncase; but not a function of actual predicted probabilities


### C-index

* c- index is the generalization of the ROC for survival data.
* Because c stat is based on ranks it is less sensitive than e.g. measures based on likelihood 

* Explanation of Method 

### Example modification 
Uno et al 2011
* C-statistic by Harell at its core is a rank correlation measure, using Kendall's tau for censored surv. data
* Problem with rank correlation: **how to order survival times in the presence of censoring**
* Brown et al.: all observations, giving prob. scores based on Kaplan Meier estimate for T
* Problem: KM not good when **covariates dependent on T**
* Alternative: **"usable pairs"**, excluding rest- Problem: **Dependency on censuring distribution** 
* Solution : **modified c-statistic which is consistent for population concordance measure, free of censoring**



* Advantages 

Great popularity because so interpretable: 

* Disadvantages 


Problem: Studies ignoring calibation (Risk prediction models in cardiovascular literature, use c-statistic, despite working with large prospective cohort studies. Nancy Coook 2007)

## Reclassification 

## Decision Analytics tools 


# Performance Evaluation metrics 

# Integrated Brier Score

## Introduction and origin story 

The score brier was intially used for weather forecasting.




## Explanation of Method 


## Advantages 


# Graf et al 1999

### Introduction 

* Point prediction of event free times inevitably gives poor results 
* Second approach is predicting the survival of event status at fixed point in time, diminishing the expected misspecification/error. 
* expected brier score may be interpreted as a mean squared error of prediction when the estimated prob, which take values in interval [0,1] are viewed as prediction of event status at t*,I(T>t*) in {0,1}.
* **Advantage**: More sophisticated to use estimated probabilities for prediction: Diagnostic test based on predictive values ; probabilities of positive or negative disease status rather than classification of diseased or not diseased. 
* Therefore: brier score, measures average discrepancies between true disease status and predicted value, better than misclassification rate

### Kattan and Gerds 2018

* Separate 'useless' and 'harmful' models
* They suggest that one would assume a harmful model (incorrectly predicting certainty) having worse score relative to a useless model (a model always predicting prevalence) that predicts with some level of predictive ability
* Further Harell's c index does not provide a value specific to the time of the horizon of prediction
* This paper suggests that performance prediction ought to be specific to the time horizon of the prediction 



## Disadvantages 

### Kattan and Gerds 2018
* **BUT**: less interpretable because requires that the performance of model is compared to the performance of the best of the useless models 
* Data dependence of reference value complicates Brier Score interpretation 
* useless benchmark depends on overall event risk 

# Conclusion 

Time to event studies require adjusted model evaluation tools for censored survival data.
At the core, studies separate between models that evaluate overall performance, discrimination and calibration.
New methods such as reclassification and clinical usefulness have gained prominence among scholarship within recent research, but did not achieve the same level of recognition among clinicians and in the applied research community. 


# References 

Antolini, L., Boracchi, P., & Biganzoli, E. (2005). A time‐dependent discrimination index for survival data. Statistics in medicine, 24(24), 3927-3944.

Cook, N. R. (2007). Use and misuse of the receiver operating characteristic curve in risk prediction. Circulation, 115(7), 928-935.

Gerds, T. A., & Schumacher, M. (2006). Consistent estimation of the expected Brier score in general survival models with right‐censored event times. Biometrical Journal, 48(6), 1029-1040.

Graf, E., Schmoor, C., Sauerbrei, W., & Schumacher, M. (1999). Assessment and comparison of prognostic classification schemes for survival data. Statistics in medicine, 18(17‐18), 2529-2545.

Kattan, M. W., & Gerds, T. A. (2018). The index of prediction accuracy: an intuitive measure useful for evaluating risk prediction models. Diagnostic and prognostic research, 2(1), 7.

Steyerberg, E. W., & Vickers, A. J. (2008). Decision curve analysis: a discussion. Medical Decision Making, 28(1), 146-149.

Steyerberg, E. W., Vickers, A. J., Cook, N. R., Gerds, T., Gonen, M., Obuchowski, N., ... & Kattan, M. W. (2010). Assessing the performance of prediction models: a framework for some traditional and novel measures. Epidemiology (Cambridge, Mass.), 21(1), 128.

Uno, H., Cai, T., Pencina, M. J., D'Agostino, R. B., & Wei, L. J. (2011). On the C‐statistics for evaluating overall adequacy of risk prediction procedures with censored survival data. Statistics in medicine, 30(10), 1105-1117.

Wang, P., Li, Y., & Reddy, C. K. (2019). Machine learning for survival analysis: A survey. ACM Computing Surveys (CSUR), 51(6), 1-36.
Chicago	
