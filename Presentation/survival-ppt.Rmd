---
title: "Model Evaluation"
subtitle: "Considerations for Time-to-Event Studies"
author: "Daniel Saggau"
date: "11/12/2020"
output:
  beamer_presentation:
    theme: "Madrid"
    colortheme: "spruce"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T)
```

## Overview

1) Time to Event Studies
2) Classical Model Evaluation: Brier Score and AUC
3) TTS Model Evaluation: IBS and c-index
4) Discussion
5) Considerations

## Time-to Event Studies 

- Diagnostic and Prognostic Study
- Working with censored data 
- Highly relevant for clinical/epidemiological studies
- In Economics e.g. to examine when a subject/borrower will default
- Survival time T, the probability of death a time point h(t,x), cumulative hazard H, and survival function S

Non-parametric hazard model (Kaplan Meier Estimator):

$h(t)= \frac{d}{dt}[log S(t)]$\newline
$S(t)= exp(-H(t))$\newline
Semi-parametric proportional hazard model (Cox Estimator):

$h(t|x\beta)=h_0(t)exp(\beta^Tx)$

## Components of Model Evaluation

(1) **Discrimination vs. Calibration (vs. Clinical Usefulness)** 

* Discrimination: Are we able to discriminate between e.g. sick and healthy patients ?
* Calibration: How concise is our prediction accuracy ?
* Clinical Usefulness: Will our model create more benefits than harm? 

(2) **Label vs. Probability**

* AUC (label based error measure via specificity and sensitivity)
* Brier Score (probability from true class label)

## Brier Score 

* Score is based on loss function at a certain point in time
* Other loss measures are the log loss or the integrated log loss
* Can Plot brier score via prediction error curves (pec)
* MSE: Scores range from 0 to infinity and closer to 0 is better 
* Brier Score: range from 0 to 1

### Formula for the Brier Score

MSE for Regression (L2 Loss):

$$\mathrm{MSE}=\frac{1}{n}\sum^n_{i=1}(y^{(i)})-\hat{y}^{(i)})^2$$



The Brier Score is the MSE for Classification: 

$$\mathrm{BS}= \frac{1}{n}\sum^n_{i=1}(\hat{\pi}(x^{(i)})-y^{(i)})^2$$

## AUC - Talking about the Curve

* Plotting TPR and TNR at different thresholds
* We integrate over all thresholds to get the AUC
* Scores range from 0 to 1 
* A higher score is better and a score of 0.5 is basically a random model

### Components of the ROC

**Sensitivity** or true positive rate:

$$\mathrm{TPF = \frac{TP}{TP+FN}}$$

**Specificity** or true negative rate:

$$\mathrm{TNR = \frac{TN}{TN+FP}}$$

## Limitations of traditional ME tools

* Working with censored data
* Working with hazards and survival function
* Account for time dependent covariates 

Early approaches:

- Excluding subjects with right censored data and only evaluate on the complete data
- Problem: Losing a lot of data and potentially inducing bias

Solution:

* Inverse of the probability of censoring weighted estimate (IPCW)

## From AUC to c-index

* AUC: *"is individual A likely to have a stroke within the next 5 years?"*
* c-index:*"is individual A or individual B more likely to have a stroke?”*
* Concordance (consistency) & discordance (inconsistency) pairs
* Kendall rank correlation coefficient test as conservative basis
* Popular assumption: right censored data

### Differentiation AUC and c-index

$$\mathrm{AUC = Pr(Risk_t(i)> Risk_t(j)| i\: has\: event\: before\: t\: and \: j \: has\: event\: after \: t})$$
$$\mathrm{C = Pr(Risk_t(i)> Risk_t(j)| i\: has\: event\: before\: t)}$$

## Example of formula for c-index 

### Formula for the c-index

$$\mathrm{\frac{Concordant\: Pairs}{Concordant\: Pairs + Discordant\: Pairs}}$$

Mathematically, we can define the c-index for time dependent covariates as: 

$$C ^{td} = \frac{\mathrm{Pr}(Risk_t(i) > Risk_t(j) \& T_i < T_j \& D_i = 1)}{\mathrm{Pr}(Ti<Tj | D_i = 1)}$$

## Integrated Brier Score (IBS)

* In e.g. 'pec' the score is called the cumulative predictive error curves
* Area under the prediction error curve
* Working with time dependent survival probabilities

### Formula for IBS (Population) 

**(integrated == T)**: 

$$L(S) = \frac{1}{NT}\displaystyle\sum^N_{i=1}\displaystyle\sum ^T_{j=1}L(S_i,t_i|t^*_j)$$

* N is the number of observations 
* $S_i$ is the predicted survival function 
* t is the time of the event 
* $t^*$ the time before event

## Implementation: Basic Setup

* Using simulated survival data with 10000 observations 
* Data entails: eventtime, censtime, time, event, X1, X2, status

\footnotesize
```{r}
set.seed(123)
library("prodlim")
library("survival")
library("pec")
dat <- SimSurv(10000)
models <- list(
  "Cox.X1" = coxph(Surv(time, status) ~ X1,
    data = dat, x = TRUE, y = TRUE),
  "Cox.X2" = coxph(Surv(time, status) ~ X2,
    data = dat, x = TRUE, y = TRUE),
  "Cox.X1.X2" = coxph(Surv(time, status) ~ X1 + X2,
    data = dat, x = TRUE, y = TRUE))
```

## Implementation: Integrated Brier Score

### IPCW based on Kaplan Meier estimates:

\footnotesize
```{r}
perror <- pec(
  object = models,
  formula = Surv(time, status) ~ 1, # ,~X1 +X2, for cox
  data = dat, exact = TRUE, cens.model = "marginal", # .model="cox"
  splitMethod = "none",
  B = 0
)
```

* **cens.model** is our ipcw estimator
* **splitMethod** is the internal validation design
* **B** is the number bootstrap samples & **M** the bootstrap size
* Optional: **cause** for competing risks
* If **exact** is equal to T then we estimate pec at all the unique values of response

## Implementation: Calibration Plot

```{r}
calPlot(models)
```

## Implementation: Prediction Error Curve

\tiny
```{r}
summary(perror, times = quantile(dat$time[dat$status == 1], c(.25, .5, .75, 1)))
```
\normalsize
* A lower score is better here
* Comparing at quantiles 
* Frequently people only use the first 3 quantifies

## Plotting the prediction error curve
\footnotesize
```{r}
plot(perror)
```

## Implementation: Cumulative Prediction Error Score (IBS)

\tiny
```{r, out.width="200px"}
crps(perror, times = quantile(dat$time[dat$status == 1], c(.25, .5, .75, 1)))
# ibs(perror,times= quantile(dat$time[dat$status==1], c(.25, .5, .75, 1)))
```
\footnotesize
* A lower score is better with scores ranging from 0 to 1
* Looking at different time points thresholds
* Score can also be derived for the individual or a specific time point in various packages

## Implementation: c-index 

### Components of the c-index function

\footnotesize
```{r}
cindex <- cindex(models,
  formula = Surv(time, status) ~ 1,
  cens.model = "marginal", data = dat,
  eval.times = quantile(dat$time[dat$status == 1], c(.25, .5, .75, 1))
)
```

* **formula** is our survival formula
* **cens.model** is our method for estimating the IPCW
* **splitMethod** is the internal validation design
* **B** the number of boostrap samples & **M** the size of the boostrap sample
* Extensions: **cause** used for competing risks

## Implementation: c-index summary value

\tiny
```{r}
cindex$response
cindex$AppCindex
cindex$time
cindex$cens.model
```

## Implementation: Measures in mlr3proba

```{r, include =F}
library("mlr3")
library("mlr3learners")
library("mlr3proba")
library("mlr3viz")

TaskSurv$new(id = "interval_censored", backend = survival::bladder2[, -c(1, 7)],
  time = "start", time2 = "stop", type = "interval2")
task <- tsk("rats")
learners <- lrns(c("surv.coxph", "surv.kaplan", "surv.ranger"))
measure <- msr("surv.graf") # for c-index you can use surv.cindex
bmr <- benchmark(benchmark_grid(task, learners, rsmp("cv", folds = 2)))
# bmr$aggregate(measure)
```

\footnotesize
```{r}
library("mlr3")
library("mlr3learners")
library("mlr3proba")
library("mlr3viz")

##' measure = msr("surv.graf") # for c-index you can use surv.cindex
##' bmr = benchmark(benchmark_grid(task, learners, rsmp("cv", folds = 3)))
##' bmr$aggregate(measure)

# Modification via: 
#' MeasureSurvGraf$new(integrated = TRUE, times, method = 2, se = FALSE)
```
\normalsize
* If integrate == T then: times =  vector of time-points over which to integrate the score; otherwise: single time point
* method ==1: Approx. to integration by dividing sample mean weighted equally 
* method ==2: Approx. to integration via mean weighted by difference between  time points (default in 'pec')


## Implementation: mlr3Proba Benchmark Example

\footnotesize
```{r, fig.width= 3.5, fig.height=2.5}
autoplot(bmr, measure = measure)
```

## Discussion 

* c-index has gained popularity because of it's interpretability
* Integrated Brier Score accounts for both calibration and discrimination
* For predictive modelling, you want to account for both components
* IBS allows for differentiation of 'useless' and 'harmful' models
* Clinical consequences problematic 

## Considerations

* Decision Curve Analysis (clinical consequences)
* Net Reclassification Improvement (clinical consequences)
* Other estimators like SVM estimators for the censored data
* Time dependent ROC/AUC

## Literature and Recommendations 

Introduction:

* Steyerberg, E. W., Vickers, A. J., Cook, N. R., Gerds, T., Gonen, M., Obuchowski, N., ... & Kattan, M. W. (2010). Assessing the performance of prediction models: a framework for some traditional and novel measures. Epidemiology (Cambridge, Mass.), 21(1), 128.

Modifications: 

* Blanche, P., Kattan, M. W., & Gerds, T. A. (2019). The c-index is not proper for the evaluation of-year predicted risks. Biostatistics, 20(2), 347-357.

* Khosla, A., Cao, Y., Lin, C. C. Y., Chiu, H. K., Hu, J., & Lee, H. (2010, July). l. In Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining (pp. 183-192).

## Use Cases:

Decision Curve Analysis: 

https://rdrr.io/github/ddsjoberg/dca/man/stdca.html

Concordance Related Model Evaluation: 

* https://rpubs.com/kaz_yos/survival-auc
* https://datascienceplus.com/time-dependent-roc-for-survival-prediction-models-in-r/
