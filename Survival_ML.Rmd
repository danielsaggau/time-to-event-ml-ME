---
title: "Time to Event Machine Learning - Evaluation"
author: "Daniel Saggau"
date: "11/9/2020"
fontsize: 11pt
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction 

Time to event studies:

* What is a time to event study?
* What problems does it create?
* Why do we need specific model evaluation metrics?


To accommodate the censored nature of our data, we need to introduce different evaluation metrics. 

This paper will introduce a number of methods, focusing on the brier score and the c-index, two methods that have gained prominence within the realms of scholarship and among clinicians.


The paper is structured as follows:
Firstly, i will introduce the integrated brier score, providing further information on the origin, theoretical underpinning and the respective characteristics.
Secondly, i will introduce the c-index, following the same procedure as with the integrated brier score.
Following the analysis of these two cornerstones of model evaluation for time to event studies, i will illustrate further competing methods and current research within the field. 
Subsequently, the fourth section will provide a brief example of how to implement given methods in R.
Lastly, the conclusion will summarize core findings of this brief comparative study. 


# Performance Evaluation metrics 

# Integrated Brier Score 

Structure: 

* Introduction and origin story 

* Explanation of Method 

* Advantages 

* Disadvantages 


## P.Wang 2017 ML for SA 

* Named after Glenn W. Brier and initially used for inaccuracy of probabilistic forecasts
* Can only be used with probabilistic outcomes [0;1, sum for each ind. is 1]
* Extended in 1999 for either binary or categorical outcomes for survival analysis
* When using censored data, we re-weight the censored information


* rare events problematic (need to research)
* uni-dimensional predictions the same as the mean squared error

## Graf et al 1999

### Introduction 

* Point prediction of event free times inevitably gives poor results 
* Second approach is predicting the survival of event status at fixed point in time, diminishing the expected misspecification/error. 
* expected brier score may be interpreted as a mean squared error of prediction when the estimated prob, which take values in interval [0,1] are viewed as prediction of event status at t*,I(T>t*) in {0,1}.
* **Advantage**: More sophisticated to use estimated probabilities for prediction: Diagnostic test based on predictive values ; probabilities of positive or negative disease status rather than classification of diseased or not diseased. 
* Therefore: brier score, measures average discrepancies between true disease status and predicted value, better than misclassification rate

### Conclusion 

* Make statement that time to event itself cannot adequately be predicted.
* Best at t=0 is try to estimate the probability that the event of interest will not occur until t*.
* Therefore, measure **needs to compare estimates of event free probabilities**
* **Criticism:** ROC methodology cannot capture feature of prognostic classification: Applies to c-index and index of concordance
* c-index has interpretation of the area under the ROC curve for an artificially constructed diagnostic test in the survival context 
* Brier and log score are well investigated and according to paper need to be implemented in SA.
* Summary measures of inaccuracy may be obtained by using loss functions integrated with respect to suitable weight function
* Introduce re-weighting scheme, leading to quantities that don't depend on censoring distribution asymptotically 

## Kattan and Gerds 2018 Use and Misuse of the ROC curve 

### Background- C-index

* Due to interpretability desired by clinicians, concordance statistics such as Harrell's c-index, and ROC curve found popularity 
* Interpretable for pairs of subjects, but seldom interest to counsel pair of patients
* Only accounts for discrimination and not calibration 
* For modeler good to have measure that isolates discrimination, however isolation requires that calibration also be assed for a comprehensive performance analysis 
* Don't separate between useless and harmful model 
* They suggest that one would assume a harmful model (incorrectly predicting certainty) having worse score relative to a useless model (a model always predicting prevalence) that predicts with some level of predictive ability

* Further Harell's c index does not provide a value specific to the time of the horizon of prediction
* This paper suggests that performance prediction ought to be specific to the time horizon of the prediction 


### Background-Brier Score 

* Overcomes limitations by distinguishing useless from harmless models 
* Brier score reflects calibration and discrimination
* Estimated specifically for time horizon
* **BUT**: less interpretable because requires that the performance of model is compared to the performance of the best of the useless models 
* Data dependence of reference value complicates Brier Score interpretation 
* useless benchmark depends on overall event risk 

### Conclusion 

* Suggests that one limitation of (index of prediction accuracy; aka calibrated brier score) IPA is that measure using average performance may not reflect improvements that affect a small subset of the population. 
* Competing risks are mentioned; claim that two patients may have different clinical decisions even when exact same predicted risk of particular event (competing risks)





## Nancy R Cook 2007, Use and Misuse of the Receiver Operating Characteristic Curve in Risk Prediction

### Introduction 

Accuracy of models can be assessed via calibration and discrimination. 
**Calibration**: Measure how well predicted probabilities agree with actual observed risk. (prediction develop disease vs actual actual of disease)
**Discrimination**: Measure of how well model can separate those who do and do not have disease of interest.(when predicts values for cases are all higher than for non-cases, we have perfect discrimination even when predicted risk does not match the proportion with disease)
* Discrimination is more interesting when we have a classification problem with patients that have and dont have prevalent disease (e.g. diagnostic testing). 
* Discrimination is measured by c-statistics/ROC

But when dealing with predictive/prognostic modeling, we need measures that calibrate and discriminate such as the brier score/R^2 and likelihood statistics.
* **Problem**: Risk prediction models in cardiovascular literature, use c-statistic, despite working with large prospective cohort studies. 


### ROC Curve and c Statistic 

* Sensitivity and specificity can be influenced e.g.
* sensitivity among molder cases of disease 
* specificity can depend on characteristics of non-cases, e.g. gender, age or prevalence of concomitant risk factors 




### Conclusion 









```{r}
library(dplyr)
library(mgcv)
library(pammtools)
library(ggplot2)
theme_set(theme_bw())
library(survival)
library(pec)

data(tumor)
## split data into train and test data
n_train   <- 400
train_idx <- sample(seq_len(nrow(tumor)), n_train)
test_idx  <- setdiff(seq_len(nrow(tumor)), train_idx)
## data transformation
tumor_ped <- as_ped(tumor[train_idx, ], Surv(days, status)~.)

# some simple models for comparison
pam1 <- pamm(
  formula = ped_status ~ s(tend) + charlson_score + age,
  data = tumor_ped)
pam2 <- pamm(
  formula = ped_status ~ s(tend) + charlson_score + age + metastases + complications,
  data = tumor_ped)
pam3 <- pamm(
  formula = ped_status ~s(tend, by = complications) + charlson_score + age +
    metastases,
  data = tumor_ped)
# calculate prediction error curves (on test data)
pec <- pec(
  list(pam1 = pam1, pam2 = pam2, pam3 = pam3),
  Surv(days, status) ~ 1, # formula for IPCW
  data = tumor[test_idx, ], # new data not used for model fit
  times = seq(.01, 1200, by = 10),
  start = .01,
  exact = FALSE
)

plot(pec)

```


# C Index 

Structure: 

* Introduction and origin story 

* Explanation of Method 

* Advantages 

* Disadvantages 



## P.Wang 2017 ML for SA 

* Common method to assess risk is to work with relative rather than absolute risk.
* C- index works with concordance probability 
*

## Antolini et al. 2005
* 


## Uno et al 2011 On the C-statistics for Evaluating Overall Adequacy of Risk Prediction Procedures with Censored Survival Data

### Introduction

* Risk score system needed for survival analysis 
* Key component is distinguishing between subjects that develop event (cases e.g. death in many survival studies) and subjects that do not (control group)
* For binary outcomes(survival/death) ROC (Receiver Operating Characteristics curve) or area under curve (AUC) have been developed 
* Authors call these methods c-statistic
* Estimation based on cond. probability for any pair of "case" & "control" the predicted risk 
* When response variable is time, we can use above 
* If not interested in time point, standard concordance measure used to evaluate overall performance of scoring system 
* Vaguely speaking, one can distinguish between methods that use loss functions (between risk score and survival time) and other based on rank correlations between these variables
* C-statistic by Harell at its core is a rank correlation measure, using Kendall's tau for censored surv. data
* Problem with rank correlation: **how to order survival times in the presence of censoring**
* Brown et al.: all observations, giving prob. scores based on Kaplan Meier estimate for T
* Problem: KM not good when **covariates dependent on T**
* Alternative: **"usable pairs"**, excluding rest- Problem: **Dependency on censuring distribution** 
* Solution : **modified c-statistic which is consistent for population concordance measure, free of censoring**

### Conclusion 




```{r}
plot(cindex(
  list(pam1 = pam1, pam2 = pam2, pam3 = pam3),
  Surv(days, status) ~ 1,
  data = tumor[test_idx, ],
  eval.times = quantile(tumor$days[tumor$status == 1], c(.25, .5, .75))))
```


```{r}

# For further information: https://mlr3book.mlr-org.com/survival.html
library("mlr3learners")
library("mlr3")
library("mlr3proba")
library("survival")
library("ranger")
library("mlr3viz")


TaskSurv$new(id = "interval_censored", backend = survival::bladder2[,-c(1, 7)],
                    time = "start", time2 = "stop", type = "interval2")

task = tsk("rats")

# some integrated learners
learners = lrns(c("surv.coxph", "surv.kaplan", "surv.ranger"))
print(learners)

??msr()
# Harrell's C-Index for survival
measure = msr("surv.cindex")
print(measure)

set.seed(1)
bmr = benchmark(benchmark_grid(task, learners, rsmp("cv", folds = 3)))
bmr$aggregate(measure)
autoplot(bmr, measure = measure)

```


```{r}
# for reference same with brier score 
# Plot after 
measure = msr("surv.brier")
print(measure)

set.seed(1)
bmr = benchmark(benchmark_grid(task, learners, rsmp("cv", folds = 3)))
bmr$aggregate(measure)
autoplot(bmr, measure = measure)

```


## Mean Absolute Error 

P.Wang 2017 ML for SA 

* Only evaluate events where the event occurred 

## Prediction error Curves

# Implementation 

# Conclusion

What is the problem?

Why is one method worse?

What is the solution?

Advice for pratical implementation?



# References 

Antolini, L., Boracchi, P., & Biganzoli, E. (2005). A time‐dependent discrimination index for survival data. Statistics in medicine, 24(24), 3927-3944.

Cook, N. R. (2007). Use and misuse of the receiver operating characteristic curve in risk prediction. Circulation, 115(7), 928-935.

Gerds, T. A., & Schumacher, M. (2006). Consistent estimation of the expected Brier score in general survival models with right‐censored event times. Biometrical Journal, 48(6), 1029-1040.

Kattan, M. W., & Gerds, T. A. (2018). The index of prediction accuracy: an intuitive measure useful for evaluating risk prediction models. Diagnostic and prognostic research, 2(1), 7.

Uno, H., Cai, T., Pencina, M. J., D'Agostino, R. B., & Wei, L. J. (2011). On the C‐statistics for evaluating overall adequacy of risk prediction procedures with censored survival data. Statistics in medicine, 30(10), 1105-1117.

Wang, P., Li, Y., & Reddy, C. K. (2019). Machine learning for survival analysis: A survey. ACM Computing Surveys (CSUR), 51(6), 1-36.
Chicago	
