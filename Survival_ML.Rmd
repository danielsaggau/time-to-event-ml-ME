---
title: "Time to Event Machine Learning - Evaluation"
author: "Daniel Saggau"
date: "11/9/2020"
fontsize: 11pt
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction 

Time to event studies:

* What is a time to event study?
* What problems does it create?
* Why do we need specific model evaluation metrics?


To accommodate the censored nature of our data, we need to introduce different evaluation metrics. 

This paper will introduce a number of methods, focusing on the brier score and the c-index, two methods that have gained prominence within the realms of scholarship and among clinicians.


Generally speaking, both measures, the c-index and the integrated brier score have their respective advantages.
While the c-index does enjoy considerably prominence among clinicians due to interpretability and the ability to make comprehensive conclusions for the individual patients.
Irrespective, the c-index only considers discrimination and does not account for calibration as an evaluation criterion which i will argue is a pivotal shortcoming when assessing different prognostic models and especially when working with machine learning methods in time to event studies.
Inevitably, this paper suggests that the integrated brier score is the more holistic model evaluation metric from the stance of a statistician. 





The paper is structured as follows:
Firstly, i will introduce the integrated brier score, providing further information on the origin, theoretical underpinning and the respective characteristics.
Secondly, i will introduce the c-index, following the same procedure as with the integrated brier score.
Following the analysis of these two cornerstones of model evaluation for time to event studies, i will illustrate further competing methods and current research within the field. 
Subsequently, the fourth section will provide a brief example of how to implement given methods in R.
Lastly, the conclusion will summarize core findings of this brief comparative study. 


# Performance Evaluation metrics 

# Integrated Brier Score 

Structure: 

* Introduction and origin story 

* Explanation of Method 

* Advantages 

* Disadvantages 


## P.Wang 2017 ML for SA 

* Named after Glenn W. Brier and initially used for inaccuracy of probabilistic forecasts
* Can only be used with probabilistic outcomes [0;1, sum for each ind. is 1]
* Extended in 1999 for either binary or categorical outcomes for survival analysis
* When using censored data, we re-weight the censored information


* rare events problematic (need to research)
* uni-dimensional predictions the same as the mean squared error

## Graf et al 1999

### Introduction 

* Point prediction of event free times inevitably gives poor results 
* Second approach is predicting the survival of event status at fixed point in time, diminishing the expected misspecification/error. 
* expected brier score may be interpreted as a mean squared error of prediction when the estimated prob, which take values in interval [0,1] are viewed as prediction of event status at t*,I(T>t*) in {0,1}.
* **Advantage**: More sophisticated to use estimated probabilities for prediction: Diagnostic test based on predictive values ; probabilities of positive or negative disease status rather than classification of diseased or not diseased. 
* Therefore: brier score, measures average discrepancies between true disease status and predicted value, better than misclassification rate

### Conclusion 

* Make statement that time to event itself cannot adequately be predicted.
* Best at t=0 is try to estimate the probability that the event of interest will not occur until t*.
* Therefore, measure **needs to compare estimates of event free probabilities**
* **Criticism:** ROC methodology cannot capture feature of prognostic classification: Applies to c-index and index of concordance
* c-index has interpretation of the area under the ROC curve for an artificially constructed diagnostic test in the survival context 
* Brier and log score are well investigated and according to paper need to be implemented in SA.
* Summary measures of inaccuracy may be obtained by using loss functions integrated with respect to suitable weight function
* Introduce re-weighting scheme, leading to quantities that don't depend on censoring distribution asymptotically 

## Kattan and Gerds 2018 Use and Misuse of the ROC curve 

### Background- C-index

* Due to interpretability desired by clinicians, concordance statistics such as Harrell's c-index, and ROC curve found popularity 
* Interpretable for pairs of subjects, but seldom interest to counsel pair of patients
* Only accounts for discrimination and not calibration 
* For modeler good to have measure that isolates discrimination, however isolation requires that calibration also be assed for a comprehensive performance analysis 
* Don't separate between useless and harmful model 
* They suggest that one would assume a harmful model (incorrectly predicting certainty) having worse score relative to a useless model (a model always predicting prevalence) that predicts with some level of predictive ability

* Further Harell's c index does not provide a value specific to the time of the horizon of prediction
* This paper suggests that performance prediction ought to be specific to the time horizon of the prediction 

### Background-Brier Score 

* Overcomes limitations by distinguishing useless from harmless models 
* Brier score reflects calibration and discrimination
* Estimated specifically for time horizon
* **BUT**: less interpretable because requires that the performance of model is compared to the performance of the best of the useless models 
* Data dependence of reference value complicates Brier Score interpretation 
* useless benchmark depends on overall event risk 

### Conclusion 

* Suggests that one limitation of (index of prediction accuracy; aka calibrated brier score) IPA is that measure using average performance may not reflect improvements that affect a small subset of the population. 
* Competing risks are mentioned; claim that two patients may have different clinical decisions even when exact same predicted risk of particular event (competing risks)










```{r}
library(dplyr)
library(mgcv)
library(pammtools)
library(ggplot2)
theme_set(theme_bw())
library(survival)
library(pec)

data(tumor)
## split data into train and test data
n_train   <- 400
train_idx <- sample(seq_len(nrow(tumor)), n_train)
test_idx  <- setdiff(seq_len(nrow(tumor)), train_idx)
## data transformation
tumor_ped <- as_ped(tumor[train_idx, ], Surv(days, status)~.)

# some simple models for comparison
pam1 <- pamm(
  formula = ped_status ~ s(tend) + charlson_score + age,
  data = tumor_ped)
pam2 <- pamm(
  formula = ped_status ~ s(tend) + charlson_score + age + metastases + complications,
  data = tumor_ped)
pam3 <- pamm(
  formula = ped_status ~s(tend, by = complications) + charlson_score + age +
    metastases,
  data = tumor_ped)
# calculate prediction error curves (on test data)
pec <- pec(
  list(pam1 = pam1, pam2 = pam2, pam3 = pam3),
  Surv(days, status) ~ 1, # formula for IPCW
  data = tumor[test_idx, ], # new data not used for model fit
  times = seq(.01, 1200, by = 10),
  start = .01,
  exact = FALSE
)

plot(pec)

```


# Concordance Index 

Structure: 

* Introduction and origin story 

* Explanation of Method 

* Advantages 

* Disadvantages 


## Nancy R Cook 2007, Use and Misuse of the Receiver Operating Characteristic Curve in Risk Prediction

### Introduction 

Accuracy of models can be assessed via calibration and discrimination. 
**Calibration**: Measure how well predicted probabilities agree with actual observed risk. (prediction develop disease vs actual actual of disease)
**Discrimination**: Measure of how well model can separate those who do and do not have disease of interest.(when predicts values for cases are all higher than for non-cases, we have perfect discrimination even when predicted risk does not match the proportion with disease)
* Discrimination is more interesting when we have a classification problem with patients that have and dont have prevalent disease (e.g. diagnostic testing). 
* Discrimination is measured by c-statistics/ROC

But when dealing with predictive/prognostic modeling, we need measures that calibrate and discriminate such as the brier score/R^2 and likelihood statistics.
* **Problem**: Risk prediction models in cardiovascular literature, use c-statistic, despite working with large prospective cohort studies. 


### ROC Curve and c Statistic 

* c- index is the generalization of the ROC for survival data.
* sensitivity of a test is the probability of a positive test result, or the value above a threshold among those with diseases.
* Specificity is the probability of a negative test result, or a value below a threshold among those without disease 
* Sensitivity and specificity can be influenced e.g.
* sensitivity among milder cases of disease 
* specificity can depend on characteristics of non-cases, e.g. gender, age or prevalence of concomitant risk factors 
* ROC is a plot of sensitivity versus 1- specificity 
* The area under the curve or the c statistic ranges from 0.5 (no discrimination) to max of 1 (perfect discrimination)
* Essentially, the c statistic is equivalent to the probability that the measure or predicted risk is higher for a case than for a noncase. 
* Further, c-statistic describes how well models rank case and noncase; but not a function of actual predicted probabilities

### c statistics and Model Selection 

* Because c stat is based on ranks it is less sensitive than e.g. measures based on likelihood 
* 

### Conclusion 


## P.Wang 2017 ML for SA 

* Common method to assess risk is to work with relative rather than absolute risk.
* C- index works with concordance probability 
*

## Antolini et al. 2005 A time-dependent discrimination index for survival data

* Extension ROC for censored data, with its biggest advantage being the interpretability 
* Suggest time dependent discrimination index 
* Ability to discriminate is summarized over time
* Grounded in following assumption: A subject who developed event should have less predicted probability of surviving beyond his survival time than any subject who survived longer 
* Prediction is focused on the individual outcome hence argue focus on accuracy of the predictions, rather than merely on the covarate effects and their statistical significance 
* Argue that **generalizability** is important and contains two levels:
* reproducability: Patients from same population  
* transportability: Patients coming from different plausibly related population 
* Assessing predictive accuracy naively model lead to overoptimistic result
* The evaluation of the degree of optimism and subsequent bias corrected predictive accuracy offers insights into true model reproducibility 
* Focus on discrimination (see 7;8 & 9 )
* c-index is discrimination measure introduced by Harrell et al. which is extension of AUC 
* AUC is considered the concordance of the ranking between the predicted probabilities of being diseased for pairs of diseased and non-diseased subjects.
* c-index is concordance between ranking of predicted failure times and that of the observed times for pairs of subjects 
* Generally speaking, if there is a one to one correspondence between predicted times and predicted survival functions, the ranking between predicted times can be obtained by opposite ranking of predicted survival prob at any fixed time point
* Using non-proportional hazard model for breast cancer patients 






## Uno et al 2011 On the C-statistics for Evaluating Overall Adequacy of Risk Prediction Procedures with Censored Survival Data

### Introduction

* Risk score system needed for survival analysis 
* Key component is distinguishing between subjects that develop event (cases e.g. death in many survival studies) and subjects that do not (control group)
* For binary outcomes(survival/death) ROC (Receiver Operating Characteristics curve) or area under curve (AUC) have been developed 
* Authors call these methods c-statistic
* Estimation based on cond. probability for any pair of "case" & "control" the predicted risk 
* When response variable is time, we can use above 
* If not interested in time point, standard concordance measure used to evaluate overall performance of scoring system 
* Vaguely speaking, one can distinguish between methods that use loss functions (between risk score and survival time) and other based on rank correlations between these variables
* C-statistic by Harell at its core is a rank correlation measure, using Kendall's tau for censored surv. data
* Problem with rank correlation: **how to order survival times in the presence of censoring**
* Brown et al.: all observations, giving prob. scores based on Kaplan Meier estimate for T
* Problem: KM not good when **covariates dependent on T**
* Alternative: **"usable pairs"**, excluding rest- Problem: **Dependency on censuring distribution** 
* Solution : **modified c-statistic which is consistent for population concordance measure, free of censoring**

### Conclusion 




```{r}
plot(cindex(
  list(pam1 = pam1, pam2 = pam2, pam3 = pam3),
  Surv(days, status) ~ 1,
  data = tumor[test_idx, ],
  eval.times = quantile(tumor$days[tumor$status == 1], c(.25, .5, .75))))
```


```{r}

# For further information: https://mlr3book.mlr-org.com/survival.html
library("mlr3learners")
library("mlr3")
library("mlr3proba")
library("survival")
library("ranger")
library("mlr3viz")


TaskSurv$new(id = "interval_censored", backend = survival::bladder2[,-c(1, 7)],
                    time = "start", time2 = "stop", type = "interval2")

task = tsk("rats")

# some integrated learners
learners = lrns(c("surv.coxph", "surv.kaplan", "surv.ranger"))
print(learners)

??msr()
# Harrell's C-Index for survival
measure = msr("surv.cindex")
print(measure)

set.seed(1)
bmr = benchmark(benchmark_grid(task, learners, rsmp("cv", folds = 3)))
bmr$aggregate(measure)
autoplot(bmr, measure = measure)

```


```{r}
# for reference same with brier score 
# Plot after 
measure = msr("surv.brier")
print(measure)

set.seed(1)
bmr = benchmark(benchmark_grid(task, learners, rsmp("cv", folds = 3)))
bmr$aggregate(measure)
autoplot(bmr, measure = measure)

```


## Integrated Area under the Curve

## Prognostic Index Curve

## Mean Absolute Error 

P.Wang 2017 ML for SA 

* Only evaluate events where the event occurred 

## Prediction error Curves

# Implementation 

# Conclusion

What is the problem?

Why is one method worse?

What is the solution?

Advice for practical implementation?



# References 

Antolini, L., Boracchi, P., & Biganzoli, E. (2005). A time‐dependent discrimination index for survival data. Statistics in medicine, 24(24), 3927-3944.

Cook, N. R. (2007). Use and misuse of the receiver operating characteristic curve in risk prediction. Circulation, 115(7), 928-935.

Gerds, T. A., & Schumacher, M. (2006). Consistent estimation of the expected Brier score in general survival models with right‐censored event times. Biometrical Journal, 48(6), 1029-1040.

Kattan, M. W., & Gerds, T. A. (2018). The index of prediction accuracy: an intuitive measure useful for evaluating risk prediction models. Diagnostic and prognostic research, 2(1), 7.

Uno, H., Cai, T., Pencina, M. J., D'Agostino, R. B., & Wei, L. J. (2011). On the C‐statistics for evaluating overall adequacy of risk prediction procedures with censored survival data. Statistics in medicine, 30(10), 1105-1117.

Wang, P., Li, Y., & Reddy, C. K. (2019). Machine learning for survival analysis: A survey. ACM Computing Surveys (CSUR), 51(6), 1-36.
Chicago	
